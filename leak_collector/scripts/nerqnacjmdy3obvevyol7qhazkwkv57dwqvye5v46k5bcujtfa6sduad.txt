from abc import ABC
from typing import List
from playwright.sync_api import Page
from crawler.crawler_instance.local_interface_model.leak.leak_extractor_interface import leak_extractor_interface
from crawler.crawler_instance.local_shared_model.data_model.entity_model import entity_model
from crawler.crawler_instance.local_shared_model.data_model.leak_model import leak_model
from crawler.crawler_instance.local_shared_model.rule_model import RuleModel, FetchProxy, FetchConfig
from crawler.crawler_services.redis_manager.redis_controller import redis_controller
from crawler.crawler_services.redis_manager.redis_enums import REDIS_COMMANDS, CUSTOM_SCRIPT_REDIS_KEYS
from crawler.crawler_services.shared.helper_method import helper_method


class _nerqnacjmdy3obvevyol7qhazkwkv57dwqvye5v46k5bcujtfa6sduad(leak_extractor_interface, ABC):
    _instance = None

    def __init__(self, callback=None):

        self.callback = callback
        self._card_data = []
        self._entity_data = []
        self.soup = None
        self._initialized = None
        self._redis_instance = redis_controller()

    def init_callback(self, callback=None):

        self.callback = callback

    def __new__(cls, callback=None):

        if cls._instance is None:
            cls._instance = super(_nerqnacjmdy3obvevyol7qhazkwkv57dwqvye5v46k5bcujtfa6sduad, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    @property
    def seed_url(self) -> str:
        return "http://nerqnacjmdy3obvevyol7qhazkwkv57dwqvye5v46k5bcujtfa6sduad.onion"

    @property
    def base_url(self) -> str:
        return "http://nerqnacjmdy3obvevyol7qhazkwkv57dwqvye5v46k5bcujtfa6sduad.onion"

    @property
    def rule_config(self) -> RuleModel:
        return RuleModel(m_fetch_proxy=FetchProxy.TOR, m_fetch_config=FetchConfig.PLAYRIGHT)

    @property
    def card_data(self) -> List[leak_model]:
        return self._card_data

    @property
    def entity_data(self) -> List[entity_model]:
        return self._entity_data

    def invoke_db(self, command: int, key: CUSTOM_SCRIPT_REDIS_KEYS, default_value):

        return self._redis_instance.invoke_trigger(command, [key.value + self.__class__.__name__, default_value])

    def contact_page(self) -> str:

        return "https://www.iana.org/help/example-domains"

    def append_leak_data(self, leak: leak_model, entity: entity_model):

        self._card_data.append(leak)
        self._entity_data.append(entity)
        if self.callback:
            self.callback()

    def parse_leak_data(self, page: Page):
        current_page = 1
        max_pages = 20
        base_pagination_url = f"{self.base_url}/?PAGEN_1="

        while current_page <= max_pages:
            try:
                # Navigate to page with extended timeout
                target_url = self.base_url if current_page == 1 else f"{base_pagination_url}{current_page}"
                page.goto(target_url, timeout=120000)
                page.wait_for_load_state("networkidle")

                # Try multiple card selectors
                card_selectors = [
                    "div.card",
                    "div.item",
                    "article",
                    "div.post",
                    "div.listing",
                    "//div[contains(@class, 'card')]"
                ]

                cards = None
                for selector in card_selectors:
                    if page.locator(selector).count() > 0:
                        cards = page.locator(selector).all()
                        print(f"Found {len(cards)} cards using selector: {selector}")
                        break

                if not cards:
                    print(f"No cards found using standard selectors on page {current_page}")
                    current_page += 1
                    continue

                for card in cards:
                    try:
                        # Extract title
                        title_selectors = ["h4", "h3", "h2", "div.title", ".heading"]
                        card_title = "Untitled"
                        for selector in title_selectors:
                            if card.locator(selector).count() > 0:
                                card_title = card.locator(selector).first.inner_text().strip()
                                break

                        # Extract content
                        content_selectors = ["div.card-body", "div.content", "div.text", "div.description"]
                        card_content = ""
                        for selector in content_selectors:
                            if card.locator(selector).count() > 0:
                                card_content = card.locator(selector).first.inner_text().strip()
                                break
                        if not card_content:
                            card_content = card.inner_text()

                        # Extract date/time (customize selectors based on actual page structure)
                        leak_date = None
                        date_selectors = [
                            "div.date",
                            "span.time",
                            "time",
                            "div.timestamp",
                            "//*[contains(text(), 'Date:')]/following-sibling::text()"
                        ]

                        for selector in date_selectors:
                            if card.locator(selector).count() > 0:
                                date_str = card.locator(selector).first.inner_text().strip()
                                try:
                                    leak_date = helper_method.parse_date(date_str)  # Use your helper method
                                    break
                                except Exception as e:
                                    print(f"Couldn't parse date '{date_str}': {e}")
                                    continue

                        # Extract other metadata if available
                        data_size = None
                        if "GB" in card_content or "MB" in card_content:
                            size_match = re.search(r"(\d+(\.\d+)?\s*(GB|MB)", card_content)
                            if size_match:
                                data_size = size_match.group(0)

                        # Create leak model with all fields
                        leak = leak_model(
                            m_title=card_title,
                            m_url=page.url,
                            m_base_url=self.base_url,
                            m_content=card_content,
                            m_important_content=card_content[:1000],  # First 1000 chars as important
                            m_network=helper_method.get_network_type(self.base_url),
                            m_section=["exploit"],  # Customize as needed
                            m_content_type=["leaks"],
                            m_screenshot="",  # Add screenshot logic if needed
                            m_weblink=[page.url],
                            m_dumplink=[],  # Add dump links if available
                            m_websites=[self.base_url],
                            m_logo_or_images=[],  # Add image URLs if available
                            m_leak_date=leak_date,
                            m_data_size=data_size,
                            m_revenue=None,  # Extract if available
                            m_password=None,  # Extract if available
                            m_status=None,  # Extract if available
                            m_views=None  # Extract if available
                        )

                        # Extract entities
                        entity = entity_model(
                            m_email_addresses=helper_method.extract_emails(card_content),
                            m_phone_numbers=helper_method.extract_phone_numbers(card_content),
                        )

                        self.append_leak_data(leak, entity)
                        print(f"Processed card: {card_title}")

                    except Exception as card_error:
                        print(f"Error processing card: {card_error}")
                        continue

                print(f"Completed page {current_page}/{max_pages}")
                current_page += 1

            except Exception as page_error:
                print(f"Error loading page {current_page}: {page_error}")
                break

        print(f"Finished scraping. Processed {current_page - 1} pages")